import pandas as pd
import numpy as np
from pathlib import Path
from argparse import ArgumentParser

import torch
from torchvision.models.segmentation import deeplabv3_resnet50
import torchvision

import cv2
import pydicom
from pydicom.pixel_data_handlers.util import  convert_color_space

from utils import segmentation_to_coordinates, process_video_with_diameter, get_coordinates_from_dicom, ybr_to_rgb

"""
This file is for 2D frame-to-frame inference.
The input is a video file (AVI, MP4, or DICOM) and the output is a video file (MP4) with the predicted frame-to-frame annotation.

Disclaimer: Please include the appropriate view corresponding to the measurement item. 
(For example, LVID is typically measured in the PLAX view and not in A4C or PSAX. 
Additionally, avoid using an excessively zoomed-out PLAX view; instead, include a standard one. The same applies to other measurement items.)
"""

#Configuration
parser = ArgumentParser()
parser.add_argument("--model_weights", type=str, required = True, choices=[
            "ivs",
            "lvid",
            "lvpw",
            "aorta",
            "aortic_root",
            "la",
            "rv_base",
            "pa",
            "ivc",
        ])
parser.add_argument("--file_path", type=str, required = True, help= "Path to the video file (AVI, MP4, or DICOM)")
parser.add_argument("--output_path", type=str, required = True, help= "Output path. Must be .mp4")
parser.add_argument("--phase_estimate", action='store_true', help="Estimate systole and diastole phase from the video. Default is False.")
args = parser.parse_args()

SEGMENTATION_THRESHOLD = 0.0
DO_SIGMOID = True

#Dicom TAG
REGION_PHYSICAL_DELTA_X_SUBTAG = (0x0018, 0x602C)
REGION_PHYSICAL_DELTA_Y_SUBTAG = (0x0018, 0x602E)

def forward_pass(inputs):
    logits = backbone(inputs)["out"] # torch.Size([1, 2, 480, 640])
    # Step 1: Apply sigmoid if needed
    if DO_SIGMOID:
        logits = torch.sigmoid(logits)
    # Step 2: Apply segmentation threshold if needed
    if SEGMENTATION_THRESHOLD is not None:
        logits[logits < SEGMENTATION_THRESHOLD] = 0.0
    # Step 3: Convert segmentation map to coordinates
    predictions = segmentation_to_coordinates(
        logits,
        normalize=False,  # Set to True if you want normalized coordinates
        order="XY"
    )
    return predictions

print(f"Note: This script is for 2D frame-to-frame inference. {args.model_weights} prediction.")

input_type = None
VIDEO_FILE = args.file_path

if VIDEO_FILE.endswith(".avi") or VIDEO_FILE.endswith(".mp4"): 
    input_type = "video"
elif VIDEO_FILE.endswith(".dcm"): 
    input_type = "dcm"

if input_type is None:
    raise ValueError("File path must be either .avi, .mp4, or .dcm")
if not args.output_path.endswith(".mp4"):
    raise ValueError("Output path must be .mp4")

# MODEL LOADING
device = "cuda:0" #cpu / cuda
weights_path = f"./weights/2D_models/{args.model_weights}_weights.ckpt"
weights = torch.load(weights_path, map_location=device)
backbone = deeplabv3_resnet50(num_classes=2)  # 39,633,986 params / num_classes should be 2
weights = {k.replace("m.", ""): v for k, v in weights.items()}
print(backbone.load_state_dict(weights)) #says <All keys matched successfully>
backbone = backbone.to(device)
backbone.eval()

frames = []
#Version VIDEO, LOAD VIDEO (AVI/MP4).
if input_type == "video":
    video = cv2.VideoCapture(VIDEO_FILE)
    if not video.isOpened():
        raise ValueError(f"Failed to open video file: {VIDEO_FILE}")
    
    while True:
        ret, frame = video.read()
        if not ret:
            break
        resized_frame = cv2.resize(frame, (640, 480))
        frames.append(resized_frame)
    video.release()
    
    if len(frames) == 0:
        raise ValueError(f"No frames were loaded from video file: {VIDEO_FILE}")
    
    frames = np.array(frames)

#Version DICOM, LOAD VIDEO (Dicom).
elif input_type == "dcm":
    ds = pydicom.dcmread(VIDEO_FILE)
    input_dicom = ds.pixel_array #Frames shape (Frame, Height, Width, Channel)
    height, width = input_dicom.shape[1], input_dicom.shape[2]
    
    doppler_region = get_coordinates_from_dicom(ds)[0]
    if REGION_PHYSICAL_DELTA_X_SUBTAG in doppler_region:
        conversion_factor_X = abs(doppler_region[REGION_PHYSICAL_DELTA_X_SUBTAG].value)
    if REGION_PHYSICAL_DELTA_Y_SUBTAG in doppler_region:
        conversion_factor_Y = abs(doppler_region[REGION_PHYSICAL_DELTA_Y_SUBTAG].value)
    
    ratio_height = height / 480 
    ratio_width = width / 640
    
    for frame in input_dicom:
        if ds.PhotometricInterpretation == "YBR_FULL_422":
            frame = ybr_to_rgb(frame)
            # frame = convert_color_space(arr=frame, current="YBR_FULL_422", desired="RGB")
            # frame =cv2.cvtColor(frame, cv2.COLOR_YUV2RGB)
        resized_frame = cv2.resize(frame, (640, 480)) 
        frames.append(resized_frame)

input_tensor = torch.tensor(frames)
print(f"Input tensor shape after conversion: {input_tensor.shape}")

if input_tensor.dim() != 4:
    raise ValueError(f"Expected 4D tensor (frames, height, width, channels), but got {input_tensor.dim()}D tensor with shape {input_tensor.shape}. "
                     f"This usually means the video frames were not loaded properly.")

input_tensor = input_tensor.float() / 255.0
input_tensor = input_tensor.to(device)
input_tensor = input_tensor.permute(0, 3, 1, 2)  # (F, C, H, W)

#In predictions, each frame-level prediction will be saved.
predictions = []
for i in range(input_tensor.shape[0]): #[0] means number of frames.
    batch = {"inputs": input_tensor[i].unsqueeze(0)} # torch.Size([1, 3, 480, 640])
    with torch.no_grad(): 
        model_output = forward_pass(batch["inputs"])
    predictions.append(model_output)
predictions = torch.cat(predictions, dim=0)
predictions = predictions.cpu().numpy()

#Make Output Video
output_video_path = args.output_path

frame_number, pred_x1s, pred_y1s, pred_x2s, pred_y2s =[], [], [], [], []
video_frames = []
  
for i, (frame, prediction) in enumerate(zip(input_tensor, predictions)):
    frame = frame.permute(1, 2, 0).cpu().numpy()
    frame = np.clip(frame, 0, 1)
    frame = (frame * 255).astype(np.uint8)
    
    #Plot points (circle)
    for point, color in zip(prediction, [(135, 206, 235), (135, 206, 235)]):
        point_0, point_1 = point[0], point[1]
        cv2.circle(frame, (int(point_0), int(point_1)), 3, color, -1)
    
    #Plot line
    cv2.line(frame, 
             (int(prediction[0][0]), int(prediction[0][1])),
             (int(prediction[1][0]), int(prediction[1][1])),
             (135, 206, 235), 
             2)
    
    pred_x1s.append(prediction[0][0])
    pred_y1s.append(prediction[0][1])
    pred_x2s.append(prediction[1][0])
    pred_y2s.append(prediction[1][1])
    frame_number.append(i)
    
    video_frames.append(torch.from_numpy(frame))

# Write video using torchvision
video_tensor = torch.stack(video_frames)  # (F, H, W, C)
torchvision.io.write_video(
    filename=output_video_path,
    video_array=video_tensor,
    fps=30,
    video_codec='libx264'
)
print(f"Done, please check {output_video_path}")

df = pd.DataFrame({
    "frame_number": frame_number,
    "pred_x1": pred_x1s,
    "pred_y1": pred_y1s,
    "pred_x2": pred_x2s,
    "pred_y2": pred_y2s,
})

df.to_csv(output_video_path.replace(".mp4", ".csv"), index=False)

if input_type == "video":
    print("Completed. Distance between two points is not calculated from video input.")

if input_type == "dcm":
    
    if args.phase_estimate and args.model_weights == "lvid":
        systole_diastole_analysis = True
    else:
        systole_diastole_analysis = False
         
    process_video_with_diameter(video_path = output_video_path, 
                                output_path = output_video_path.replace(".mp4", "_distance.mp4"),
                                conversion_factor_X = conversion_factor_X,
                                conversion_factor_Y = conversion_factor_Y,
                                df = df,
                                ratio = ratio_height,
                                systole_diastole_analysis = systole_diastole_analysis
                                )
    print("Distance between two points is calculated from .dcm input.")
    print(f"Completed.Please check {output_video_path.replace('.mp4', '_distance.mp4')}")

#SAMPLE SCRIPT

#python inference_2D_image.py --model_weights "ivs" 
#--file_path "./SAMPLE_DICOM/IVS_SAMPLE_0.dcm" 
#--output_path "./OUTPUT/MP4/IVS_SAMPLE_GENERATED.mp4"

#python inference_2D_image.py --model_weights "lvid" 
#--file_path "./SAMPLE_DICOM/LVID_SAMPLE_0.dcm" 
#--output_path "./OUTPUT/MP4/LVID_SAMPLE_GENERATED.mp4"

#python inference_2D_image.py --model_weights "lvpw"
#--file_path "./SAMPLE_DICOM/LVPW_SAMPLE_0.dcm"
#--output_path "./OUTPUT/MP4/LVPW_SAMPLE_GENERATED.mp4"

#python inference_2D_image.py --model_weights "aorta"
#--file_path "./SAMPLE_DICOM/AORTA_SAMPLE_0.dcm"
#--output_path "./OUTPUT/MP4/AORTA_SAMPLE_GENERATED.mp4"

#python inference_2D_image.py --model_weights "aortic_root"
#--file_path "./SAMPLE_DICOM/AORTIC_ROOT_SAMPLE_0.dcm"
#--output_path "./OUTPUT/MP4/AORTIC_ROOT_SAMPLE_GENERATED.mp4"

#python inference_2D_image.py --model_weights "la"
#--file_path "./SAMPLE_DICOM/LA_SAMPLE_0.dcm"
#--output_path "./OUTPUT/MP4/LA_SAMPLE_GENERATED.mp4"

#python inference_2D_image.py --model_weights "rv_base"
#--file_path "./SAMPLE_DICOM/RV_BASE_SAMPLE_0.dcm"
#--output_path "./OUTPUT/MP4/RV_BASE_SAMPLE_GENERATED.mp4"

#python inference_2D_image.py --model_weights "pa"
#--file_path "./SAMPLE_DICOM/PA_SAMPLE_0.dcm"
#--output_path "./OUTPUT/MP4/PA_SAMPLE_GENERATED.mp4"

#python inference_2D_image.py --model_weights "ivc"
#--file_path "./SAMPLE_DICOM/IVC_SAMPLE_0.dcm" 
#--output_path "./OUTPUT/MP4/IVC_SAMPLE_GENERATED.mp4" 



